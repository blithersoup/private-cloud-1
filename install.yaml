- hosts: main
  tasks:
    - name: Install ubuntu gpu drivers
      ansible.builtin.shell: ubuntu-drivers autoinstall
      become: yes
      become_user: root

    - name: Disable ufw
      shell: ufw disable
      become: yes
    
    - name: disable swap
      shell: swapoff -a
      become: yes
    
    - name: Update APT package cache
      ansible.builtin.apt:
        update_cache: yes
      become: true

    - name: Upgrade all APT packages
      ansible.builtin.apt:
        upgrade: dist
      become: true

    - name: Import NVIDIA GPG key to apt trusted keys
      ansible.builtin.shell:
        cmd: >
          curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        executable: /bin/bash
        creates: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      become: true
      
    - name: Add NVIDIA APT repository list
      ansible.builtin.shell: 
        cmd: >
          curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list |
          sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' |
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        executable: /bin/bash
      become: true

    - name: Update APT package cache
      ansible.builtin.apt:
        update_cache: yes
      become: true
    
    - name: Install nvidia container toolkit and networking utils
      ansible.builtin.apt:
        name: "{{ item }}"
        state: present
        update_cache: yes
      become: true
      loop:
        - zfsutils-linux
        - nfs-kernel-server
        - cifs-utils
        - open-iscsi
        - avahi-daemon
        - libnss-mdns
        - python3-kubernetes
        - nvidia-container-toolkit
    
    - name: Ensure avahi-daemon is enabled on boot
      ansible.builtin.systemd:
        name: avahi-daemon
        enabled: true
      become: true

    - name: Reboot for changes to take effect
      ansible.builtin.reboot:
        pre_reboot_delay: 0
        reboot_timeout: 600
      become: true
    
    - name: Create setup clustertoken
      shell: head /dev/urandom | tr -dc A-Za-z0-9 | head -c 32 ; echo ''
      register: clustertoken

    - name: Download and install K3s with no flannel, network policy, or kube-proxy
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | sh -
      environment:
        K3S_TOKEN: "{{ clustertoken.stdout }}"
        INSTALL_K3S_EXEC: "--flannel-backend=none --disable-network-policy --disable-kube-proxy"
      become: true

    - name: Ensure ~/.kube directory exists
      become: false
      ansible.builtin.file:
        path: "~/.kube"
        state: directory
        mode: '0755'

    - name: Export kubeconfig from K3s
      ansible.builtin.command: k3s kubectl config view --raw
      become: true
      register: kubeconfig_output

    - name: Write kubeconfig to user home
      become: false
      ansible.builtin.copy:
        content: "{{ kubeconfig_output.stdout }}"
        dest: "~/.kube/config"
        mode: '0600'

    - name: Ensure KUBECONFIG env var is in .bashrc
      become: false
      ansible.builtin.lineinfile:
        path: "/home/{{ ansible_user }}/.bashrc"
        line: "export KUBECONFIG=$HOME/.kube/config"
        insertafter: EOF

    - name: Ensure kubectl completion is sourced in .bashrc
      become: false
      ansible.builtin.lineinfile:
        path: "/home/{{ ansible_user }}/.bashrc"
        line: "source <(kubectl completion bash)"
        insertafter: EOF
    
    - name: Get ip adr
      shell: hostname -I | awk '{print $1}'
      register: ipaddr

- hosts: agent
  tasks:
    - name: Update APT package cache
      ansible.builtin.apt:
        update_cache: yes
      become: true
    
    - name: Disable ufw
      shell: ufw disable
      become: true

    - name: disable swap
      shell: swapoff -a
      become: true

    - name: Install packages
      ansible.builtin.apt:
        name: "{{ item }}"
        state: present
        update_cache: yes
      become: true
      loop:
        - zfsutils-linux
        - nfs-kernel-server
        - cifs-utils
        - open-iscsi
        - avahi-daemon
        - libnss-mdns

    - name: Ensure avahi-daemon is enabled on boot
      ansible.builtin.systemd:
        name: avahi-daemon
        enabled: true
      become: true

    - name: Reboot for changes to take effect
      ansible.builtin.reboot:
        pre_reboot_delay: 0
        reboot_timeout: 600
      become: true
    
    - debug:
        msg: "The IP address of the main node is {{ hostvars[groups['main'][0]]['ipaddr'].stdout }} and the clustertoken is {{ hostvars[groups['main'][0]]['clustertoken'] }}"  
    
    - name: Download and install K3s agent
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | sh -
      environment:
        K3S_TOKEN: "{{ hostvars[groups['main'][0]]['clustertoken'].stdout }}"
        K3S_URL: "https://{{ hostvars[groups['main'][0]]['ipaddr'].stdout }}:6443"
      become: true

- hosts: main
  vars:
    cilium_cli_version: "0.18.3"
    hubble_cli_version: "1.17.3"
    arch_map:
      x86_64: amd64
      aarch64: arm64
  collections:
    - community.kubernetes
    - kubernetes.core
  tasks:
    - name: Clone private-cloud-1 repo
      ansible.builtin.git:
        repo: https://github.com/blithersoup/private-cloud-1.git
        dest: /home/{{ ansible_user }}/private-cloud-1
        clone: yes
        update: yes

    - name: Install helm
      unarchive:
        src: https://get.helm.sh/helm-v3.18.0-linux-amd64.tar.gz
        dest: /usr/local/bin
        extra_opts: "--strip-components=1"
        owner: root
        group: root
        mode: 0755
        remote_src: true
      args:
        creates: /usr/local/bin/helm
      become: true

    - name: Ensure gpu-operator namespace exists
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: gpu-operator
        state: present

    - name: Apply NVIDIA timeslice config
      kubernetes.core.k8s:
        state: present
        namespace: gpu-operator
        src: "/home/{{ ansible_user }}/private-cloud-1/config/nvidia/timeslice.yaml"
    
    - name: Ensure Helm repository cilium is added
      community.kubernetes.helm_repository:
        name: cilium
        repo_url: https://helm.cilium.io/
        state: present

    - name: Update helm repos
      shell: helm repo update
  
    - name: Install Cilium via Helm chart
      helm:
        name: cilium
        chart_ref: cilium/cilium
        chart_version: "1.17.4"
        release_namespace: kube-system
        values:
          k8sServiceHost: "{{ ipaddr.stdout }}"
          k8sServicePort: 6443
          cluster:
            name: private-cloud
            id: 1
          ipam:
            mode: cluster-pool
            operator:
              clusterPoolIPv4PodCIDRList: "10.42.0.0/16"
          hubble:
            enabled: true
            relay:
              enabled: true
            ui:
              enabled: true
          routingMode: native
          kubeProxyReplacement: true
          socketLB:
            hostNamespaceOnly: true
          enableIPv4Masquerade: true
          ipv4NativeRoutingCIDR: "10.42.0.0/16"
          ipMasqAgent:
            enabled: false
          bpf:
            masquerade: true
            hostLegacyRouting: true
          autoDirectNodeRoutes: true

    - name: Set CLI architecture
      set_fact:
        cli_arch: "{{ arch_map[ansible_architecture] }}"

    - name: Download Cilium CLI
      get_url:
        url: "https://github.com/cilium/cilium-cli/releases/download/v{{ cilium_cli_version }}/cilium-linux-{{ cli_arch }}.tar.gz"
        dest: "/tmp/cilium.tar.gz"
        mode: '0644'
      become: true

    - name: Extract and install Cilium CLI
      unarchive:
        src: "/tmp/cilium.tar.gz"
        dest: "/usr/local/bin/"
        remote_src: yes
        extra_opts: [--strip-components=0]
      become: true

    - name: Ensure cilium is executable
      file:
        path: "/usr/local/bin/cilium"
        mode: '0755'

    - name: Download Hubble CLI
      get_url:
        url: "https://github.com/cilium/hubble/releases/download/v{{ hubble_cli_version }}/hubble-linux-{{ cli_arch }}.tar.gz"
        dest: "/tmp/hubble.tar.gz"
        mode: '0644'
      become: true

    - name: Extract and install Hubble CLI
      unarchive:
        src: "/tmp/hubble.tar.gz"
        dest: "/usr/local/bin/"
        remote_src: yes
        extra_opts: [--strip-components=0]
      become: true

    - name: Ensure hubble is executable
      file:
        path: "/usr/local/bin/hubble"
        mode: '0755'
    
    - name: wait for cilium to be ready
      command: cilium status --wait
      register: cilium_status
    
    - name: Ensure Helm repository nvidia is added
      community.kubernetes.helm_repository:
        name: nvidia
        repo_url: https://helm.ngc.nvidia.com/nvidia
        state: present

    - name: Update helm repos
      shell: helm repo update

    - name: Install GPU Operator with values file
      community.kubernetes.helm:
        name: gpu-operator
        chart_ref: nvidia/gpu-operator
        release_namespace: gpu-operator
        create_namespace: false
        values_files:
          - /home/{{ ansible_user }}/private-cloud-1/config/nvidia/values.yaml
        wait: true
        timeout: 10m
        state: present

    - name: Add Tailscale Helm repo
      helm_repository:
        name: tailscale
        repo_url: https://pkgs.tailscale.com/helmcharts

    - name: Install tailscale-operator via Helm chart
      helm:
        name: tailscale-operator
        chart_ref: tailscale/tailscale-operator
        release_namespace: tailscale
        create_namespace: true
        state: present
        values:
          oauth:
            clientId: "{{ tailscale_client_id }}"
            clientSecret: "{{ tailscale_client_secret }}"

    - name: Ensure Helm repository argocd is added
      community.kubernetes.helm_repository:
        name: argo
        repo_url: https://argoproj.github.io/argo-helm
        state: present

    - name: Update helm repos
      shell: helm repo update

    - name: Install Argo CD with values file
      community.kubernetes.helm:
        name: argocd
        chart_ref: argo/argo-cd
        release_namespace: argocd
        create_namespace: true
        wait: true
        timeout: 10m
        values_files:
          - /home/{{ ansible_user }}/private-cloud-1/config/argocd/values.yaml
        state: present

    - name: Get argocd-initial-admin-secret
      kubernetes.core.k8s_info:
        kind: Secret
        namespace: argocd
        name: argocd-initial-admin-secret
      register: argocd_secret

    - name: Extract and decode Argo CD admin password
      set_fact:
        argocd_admin_password: "{{ argocd_secret.resources[0].data.password | b64decode }}"

    - name: Show Argo CD admin password
      debug:
        msg: "Argo CD admin password is: {{ argocd_admin_password }}"
    
    - name: Apply Argo CD Application manifest
      kubernetes.core.k8s:
        state: present
        namespace: argocd
        src: /home/{{ ansible_user }}/private-cloud-1/argocd-apps/application.yaml
